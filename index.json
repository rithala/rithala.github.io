[{"content":"After almost two years, I am happy to announce the new version of Power Automate Tools. It introduces important fixes and editing of connection references, which allows easy flow copying.\nv1.2 Fixed the issue of saving a flow Support for launching from the new Power Automate designer Improved launching from the Power Apps Portal Now the editor allows to edit the flow definition and connection references Roadmap I am planning to implement the following features in the upcoming months:\nOpen extension from anywhere in the \u0026ldquo;make Power Apps/Automate\u0026rdquo; portal without a selected flow Open multiple tabs with different flows Browse flows from the environment directly in the extension Support for flow versioning known from the new designer Local flow versioning without saving it to an environment Snippets notebook. Save a selection for later use. You can learn more about the extension on the GitHub page, Chrome Web Store or on the Edge Add-ons listing.\n","permalink":"https://rithala.github.io/posts/power-automate-tools-version-v-1-2/","summary":"After almost two years, I am happy to announce the new version of Power Automate Tools. It introduces important fixes and editing of connection references, which allows easy flow copying.\nv1.2 Fixed the issue of saving a flow Support for launching from the new Power Automate designer Improved launching from the Power Apps Portal Now the editor allows to edit the flow definition and connection references Roadmap I am planning to implement the following features in the upcoming months:","title":"New Power Automate Tools Version 1.2"},{"content":"This is another post about generating an API client. Creating API clients is one of those tasks that should be automated as much as possible. In most cases, you just write the same code over and over by replicating the API schema. Previously, I was talking about generating Power Platform custom connectors based on an Open API definition. This time let\u0026rsquo;s talk about doing it in TypeScript and using it in an SPFx project.\nOpenAPI Typescript Codegen NPM package You can find many packages that can generate API clients in TypeScript. I especially like this one. It is quite flexible, supports many HTTP client libraries, and you can easily add your authentication to it. Last but not least, it takes as much information from an Open API as it can. This tool even adds JSDoc comments to the client methods!\nHow to use it? First, you must install the tool in your project.\nnpm install openapi-typescript-codegen --save-dev Generating the client Now it is time to generate the client. We will take a slightly modified base ASP.NET Web API project with the weather forecast controller.\nEnsure there are operation ids as they will be your client\u0026rsquo;s methods names. Otherwise, the generator concatenates the HTTP method name and path. This might result in some ugly names. Also, it is nice to have methods and parameter descriptions in place. It will be used to document the client\u0026rsquo;s methods.\nSave the API definition to a JSON file and run the generator command.\nnpx openapi-typescript-codegen --input ./apiDef.json -o ./src/api-client/generated The generator populates the output directory with its core models, API models, and services for path groups (tags).\nThe file that is most important for us is WeatherForecastService.ts. Let\u0026rsquo;s have a look at what you can find inside.\n// WeatherForecastService.ts // Imports omitted for brevity export class WeatherForecastService { /** * Get weather forecast * @returns WeatherForecast Success * @throws ApiError */ public static getWeatherForecast(): CancelablePromise\u0026lt;Array\u0026lt;WeatherForecast\u0026gt;\u0026gt; { return __request(OpenAPI, { method: \u0026#39;GET\u0026#39;, url: \u0026#39;/WeatherForecast\u0026#39;, }); } /** * Get weather forecast by city * @param cityName City name * @returns WeatherForecast Success * @throws ApiError */ public static getWeatherForecastByCity( cityName: string, ): CancelablePromise\u0026lt;Array\u0026lt;WeatherForecast\u0026gt;\u0026gt; { return __request(OpenAPI, { method: \u0026#39;GET\u0026#39;, url: \u0026#39;/WeatherForecast/{cityName}\u0026#39;, path: { \u0026#39;cityName\u0026#39;: cityName, }, }); } } It is super clean and simple. Now just set the base URL for the API and use one of the methods to get the data.\nimport { OpenAPI, WeatherForecastService } from \u0026#39;./generated\u0026#39;; OpenAPI.BASE = \u0026#39;https://your-api.com/api\u0026#39;; const forecast = await WeatherForecastService.getWeatherForecast(); Alright, you should end up with a fully functional client.\nYou can simplify generating the API client by creating an NPM script to save some time during the next runs.\n// package.json \u0026#34;scripts\u0026#34; : { \u0026#34;generate-client\u0026#34;: \u0026#34;openapi-typescript-codegen --input ./temp/apiDef.json -o ./src/api-client/generated\u0026#34;, \u0026#34;pregenerate-client\u0026#34;: \u0026#34;curl https://your-api.com/swagger.json -L --output ./temp/apiDef.json\u0026#34; // ... } Then when you run npm run generate-client the script downloads the latest API schema and then generates the client.\nAuthentication Probably you have already wondered how to modify the request before sending it to add the JWT token to the headers. It is pretty simple. The client has the OpenAPI configuration object. One of its properties is TOKEN. Which can be a string or a function that returns the promise with the token.\nimport { OpenAPI } from \u0026#39;./generated\u0026#39;; import { OAuth } from \u0026#39;./oauth-client\u0026#39;; OpenAPI.BASE = \u0026#39;https://your-api.com/api\u0026#39;; OpenAPI.TOKEN = async (options) =\u0026gt; await OAuth.getToken(); Use the client inside an SPFx project You got the API client ready and you know how to add an authentication token. The next step is to wire all things together with the SPFx context.\nYou told the generator to put the client into src/api-client/generated. This allows you to separate your code from the generated one and keep simple imports. Additionally, the next client updates will not break your client configuration. Add a new file under src/api-client/index.ts with the following content.\nimport { OpenAPI } from \u0026#39;./generated\u0026#39;; import { BaseComponentContext } from \u0026#39;@microsoft/sp-component-base\u0026#39;; export * from \u0026#39;./generated\u0026#39;; export async function setupApiClient( spfxContext: BaseComponentContext, apiUrl: string, resourceEndpoint: string) { // Get AAD token provider const tokenProvider = await spfxContext.aadTokenProviderFactory.getTokenProvider(); // Set base API url OpenAPI.BASE = apiUrl; // Get token for provided resource endpoint (e.g. api://934ac834-9390-49ce-9fa5-ffdbca6ba3f1). // The second parameter indicates that the token should be cached OpenAPI.TOKEN = async (options) =\u0026gt; await tokenProvider.getToken(resourceEndpoint, true); } Execute the created function inside the SPFx component onInit() method.\n// HelloWorldWebPart.ts export default class HelloWorldWebPart extends BaseClientSideWebPart\u0026lt;IHelloWorldWebPartProps\u0026gt; { // ... protected async onInit(): Promise\u0026lt;void\u0026gt; { await super.onInit(); await setupApiClient(this.context, this.properties.apiUrl, this.properties.resourceEndpoint); } } Since then you can import one of the services and use it inside your SPFx customization.\nimport { WeatherForecastService } from \u0026#39;../api-client\u0026#39;; const forecast = await WeatherForecastService.getWeatherForecast(); Summary Just one package and a few lines of code may greatly improve your experience in working with custom APIs inside an SPFx (not only) project.\nHappy coding!\n","permalink":"https://rithala.github.io/posts/generate-typescript-api-client-and-use-it-in-spfx-project/","summary":"This is another post about generating an API client. Creating API clients is one of those tasks that should be automated as much as possible. In most cases, you just write the same code over and over by replicating the API schema. Previously, I was talking about generating Power Platform custom connectors based on an Open API definition. This time let\u0026rsquo;s talk about doing it in TypeScript and using it in an SPFx project.","title":"How To Generate the TypeScript API Client and Use It in an SPFx Project"},{"content":"I remember when I first used a GraphQL API in a React SPA. It was a totally different experience from the one I was used to. The flexibility and ease of API discovery with GraphiQL blew my mind. I was using and creating ODATA APIs by then and my first thought was these must be pretty much the same. In fact, the concept is similar. Give front-end developers an API that will allow them to make requests as they want, and get only the data that they want. However, the approach is very different.\nSince then I had a few tries with building GraphQL APIs in .NET using Hot Chocolate or GraphQL .NET. None of them convinced me to change the approach to how I build my APIs.\nRecently, I started a project where we are building the Node.js API that provides data from an existing SQL database. Our team chose to use Prisma ORM to generate the database client. When I was looking for more information about this ORM I found out that it plays nicely with GraphQL servers and libraries. It can even generate GraphQL schema and resolvers based on a database schema. Then I recalled my past GraphQL fascination and decided to give it a shot.\nIn this article, I am building a PoC that even goes further. Let\u0026rsquo;s make this API serverless by hosting it on Azure Function App.\nHere you can find the source code of this demo app.\nWhy do that? Why not :) There are many use cases when you need to deliver quickly the API for existing data. Or you want to focus on the database design and the API just transports data back and forth between clients and a database. This is also a good starting point for developing a new API.\nFor sure in a real-world scenario, there is much more that has to be done like authorization, side effects or integrations with other systems.\nPrerequisites Node.js LTS Azure Functions Core Tools Database, you can use the academic classic\u0026rsquo;s Northwind Azure CLI or Azure Functions VS Code extension to deploy the app to Azure Prepare database First, you need a database. In this example, I use the Northwind sample database hosted in Azure. Here is the official quickstart guide. You can create a simple DTU-based basic single instance for a testing purpose. Remember to enable SQL authentication in the SQL server configuration. For now, Prisma does not support Azure AD authentication. Note down the server name, database name, admin user name and password. You will need this to create the connection URL.\nNow you can populate the database. Copy the SQL from Azure SQL Northwind sample and execute on the created database using SQL Management Studio, Azure Data Studio or just simply in Azure Portal using Query Editor in the database resource. If you are connecting for the first time you will be asked to whitelist the IP address in the Azure SQL server firewall settings.\nThe script execution may take a while. The created database schema should look like this.\nAzure Function project setup I recommend using the Azure Functions VS Code extension to create the project.\nPress CTRL + SHIFT + P type az and select create a new project. Use the current dictionary or select a different one. Select TypeScript. Now you can select the HTTP trigger function to create. Provide the name. This step is quite important. If you want to secure your API yourself or use the App Service built-in authentication (Easy Auth) then choose the \u0026ldquo;Anonymous\u0026rdquo; authorization level. In this demo, I selected the \u0026ldquo;Function\u0026rdquo; level for simplicity. You will use the function key in the \u0026ldquo;x-functions-key\u0026rdquo; header to authenticate requests. Installing dependencies There is a list of libraries that have to be installed. The most important ones are:\nprisma - previously described ORM graphql@15 - GraphQL language runtime. Version 15 is required by other used packages. type-graphql - Creating GraphQL schema using TypeScript decorators. typegraphql-prisma - Generating type-graphql classes based on Prisma model. apollo-server-azure-functions - Host Apollo (popular GraphQL server) on an Azure Function App. reflect-metadata - Metadata reflection polyfill. You can use two following commands to install all required dependencies.\nnpm i apollo-server-azure-functions reflect-metadata graphql@15 graphql-fields graphql-scalars type-graphql class-validator @prisma/client npm i -D prisma typegraphql-prisma @types/graphql-fields TypeScript configuration TypeGraphQL package requires additional TypeScript configuration to work correctly.\nSo you need to change the tsconfig.json file from this:\n{ \u0026#34;compilerOptions\u0026#34;: { \u0026#34;module\u0026#34;: \u0026#34;commonjs\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;es6\u0026#34;, \u0026#34;outDir\u0026#34;: \u0026#34;dist\u0026#34;, \u0026#34;rootDir\u0026#34;: \u0026#34;.\u0026#34;, \u0026#34;sourceMap\u0026#34;: true, \u0026#34;strict\u0026#34;: false } } To make it look like this:\n{ \u0026#34;compilerOptions\u0026#34;: { \u0026#34;module\u0026#34;: \u0026#34;commonjs\u0026#34;, \u0026#34;outDir\u0026#34;: \u0026#34;dist\u0026#34;, \u0026#34;rootDir\u0026#34;: \u0026#34;.\u0026#34;, \u0026#34;sourceMap\u0026#34;: true, \u0026#34;strict\u0026#34;: false, \u0026#34;target\u0026#34;: \u0026#34;es2018\u0026#34;, \u0026#34;lib\u0026#34;: [\u0026#34;es2018\u0026#34;, \u0026#34;esnext.asynciterable\u0026#34;], \u0026#34;esModuleInterop\u0026#34;: true, \u0026#34;experimentalDecorators\u0026#34;: true, \u0026#34;emitDecoratorMetadata\u0026#34;: true } } Prisma project initialization Prisma needs its configuration. The easiest way to start is to run the CLI command.\nnpx prisma init --datasource-provider sqlserver This creates two files. The prisma/schema.prisma schema file and .env for keeping the design-time database URL.\nYou should have kept the values that I mentioned during creating the database. Edit .env file using those values.\nDATABASE_URL=sqlserver://{server name}.database.windows.net:1433;database={database name};user={admin user name};password={admin password};encrypt=true Also, you need to change the Prisma schema file to include the GraphQL generator in it. Let\u0026rsquo;s change prisma/schema.prisma to include this.\ngenerator client { provider = \u0026#34;prisma-client-js\u0026#34; } generator typegraphql { provider = \u0026#34;typegraphql-prisma\u0026#34; output = \u0026#34;../generated/type-graphql\u0026#34; } datasource db { provider = \u0026#34;sqlserver\u0026#34; url = env(\u0026#34;DATABASE_URL\u0026#34;) } The next steps are to pull the database schema and generate the database client and GraphQL schema. Run these commands.\nnpx prisma db pull npx prisma generate If the command run successfully you should see many files generated under generated/type-graphql folder.\nGraphQL function Finally, it is time to wire all things together and test the app. The first step is to make some changes to the function definition located in GraphQL/function.json.\n{ \u0026#34;bindings\u0026#34;: [ { \u0026#34;authLevel\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;httpTrigger\u0026#34;, \u0026#34;direction\u0026#34;: \u0026#34;in\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;req\u0026#34;, \u0026#34;methods\u0026#34;: [\u0026#34;get\u0026#34;, \u0026#34;post\u0026#34;, \u0026#34;options\u0026#34;] }, { \u0026#34;type\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;direction\u0026#34;: \u0026#34;out\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;$return\u0026#34; } ], \u0026#34;scriptFile\u0026#34;: \u0026#34;../dist/GraphQL/index.js\u0026#34; } The difference between the default configurations is out the binding name changed to $return to take the output from the returned value, and added options to methods to support CORS.\nReplace the GraphQL/intex.ts content with the following code.\nimport \u0026#39;reflect-metadata\u0026#39;; import { PrismaClient } from \u0026#39;@prisma/client\u0026#39;; import { buildSchema } from \u0026#39;type-graphql\u0026#39;; import { AzureFunction, Context, HttpRequest } from \u0026#39;@azure/functions\u0026#39;; import { ApolloServer } from \u0026#39;apollo-server-azure-functions\u0026#39;; import { resolvers } from \u0026#39;../generated/type-graphql\u0026#39;; async function getClient() { const prisma = new PrismaClient(); await prisma.$connect(); return prisma; } async function buildGraphQlSchema() { return await buildSchema({ resolvers, validate: false, }); } // Lazy initialization of Prisma client and GraphQL schema const prismaClientPromise = getClient(); const graphqlSchemaPromise = buildGraphQlSchema(); async function buildApolloHandler() { const schema = await graphqlSchemaPromise; const prismaClient = await prismaClientPromise; const server = new ApolloServer({ schema: schema, csrfPrevention: true, cache: \u0026#39;bounded\u0026#39;, // enabled introspection to test the deployed function in Apollo Studio Sandbox introspection: true, context: { prisma: prismaClient, }, }); return server.createHandler({ disableHealthCheck: true, }); } // Lazy initialization of Apollo Azure Function handler const apolloHandlerPromise = buildApolloHandler(); export const graphqlHandler: AzureFunction = async function ( context: Context, req: HttpRequest ): Promise\u0026lt;void\u0026gt; { // Resolve Apollo handler instance const handler = await apolloHandlerPromise; return await new Promise((resolve, reject) =\u0026gt; { const contextWithPromisifiedDone = { ...context, // This hack is required as we require some async operation prior using handler. done: (err?: Error | string | null, result?: any) =\u0026gt; { if (err) { reject(err); return; } resolve(result); }, }; // Run Apollo handler handler(contextWithPromisifiedDone, req); }); }; Ok, try to run the app.\nnpm run start Unfortunately, quickly after the app starts the error message appears.\nUnhandledPromiseRejectionWarning: Error: Some errors occurred while generating GraphQL schema: Input Object type CustomerCustomerDemoUpdateManyMutationInput must define one or more fields. Input Object type EmployeeTerritoriesUpdateManyMutationInput must define one or more fields. After short googling, you can find out this is related to Prisma\u0026rsquo;s issue#4004. For now, there is no well no workaround. You can try to automate finding empty input classes and add some dummy property. In this case, I added a dummy property to CustomerCustomerDemoUpdateManyMutationInput.ts and EmployeeTerritoriesUpdateManyMutationInput.ts manually.\n// generated/type-graphql/resolvers/inputs/CustomerCustomerDemoUpdateManyMutationInput.ts import * as TypeGraphQL from \u0026#39;type-graphql\u0026#39;; import * as GraphQLScalars from \u0026#39;graphql-scalars\u0026#39;; import { Prisma } from \u0026#39;@prisma/client\u0026#39;; import { DecimalJSScalar } from \u0026#39;../../scalars\u0026#39;; @TypeGraphQL.InputType(\u0026#39;CustomerCustomerDemoUpdateManyMutationInput\u0026#39;, { isAbstract: true, }) export class CustomerCustomerDemoUpdateManyMutationInput { @TypeGraphQL.Field((_type) =\u0026gt; String, { nullable: false, }) DoNotUseThisInputType!: string; } // generated/type-graphql/resolvers/inputs/EmployeeTerritoriesUpdateManyMutationInput.ts import * as TypeGraphQL from \u0026#39;type-graphql\u0026#39;; import * as GraphQLScalars from \u0026#39;graphql-scalars\u0026#39;; import { Prisma } from \u0026#39;@prisma/client\u0026#39;; import { DecimalJSScalar } from \u0026#39;../../scalars\u0026#39;; @TypeGraphQL.InputType(\u0026#39;EmployeeTerritoriesUpdateManyMutationInput\u0026#39;, { isAbstract: true, }) export class EmployeeTerritoriesUpdateManyMutationInput { @TypeGraphQL.Field((_type) =\u0026gt; String, { nullable: false, }) DoNotUseThisInputType!: string; } Second try with npm run start and now without errors! Open the following URL in the browser http://localhost:7071/api/GraphQL. You should be redirected to Apollo Studio Sandbox. Now you can explore the API schema.\nThe generated API is quite rich with queries and mutations. The API supports full CRUD operations. You can get, create, update, remove, get rows with related entities, filter them or even do aggregations like COUNT, AVG, SUM, MAX or MIN.\nPublish the function app to Azure First, you need to create the function app in the resource group. I tested it on the Windows consumption plan only.\nAfter the function is created go to the created resource and open the \u0026ldquo;Configuration\u0026rdquo; pane. You got to add the \u0026ldquo;DATABASE_URL\u0026rdquo; to the application settings. Take the value from the .env file.\nThe last thing to do here is to select the \u0026ldquo;General settings\u0026rdquo; tab and change the platform to 64 Bit. Prisma binaries only work with 64-bit platforms. Remember to save changes before leaving the configuration.\nTo use this API in Apollo Studio you have to change the CORS settings. Go to API -\u0026gt; CORS, remove the default rule and add a new one. As you are already here, go to the Functions -\u0026gt; App keys pane and note down the default key.\nIt is time to go back to VS Code and deploy the code to Azure. Again, press CTRL + SHIFT + P type az and select Azure Functions: Deploy to Function App. You might be asked to log in, then chose the target subscription and previously created function app. Confirm that you are aware that deployment will erase the target function app.\nIt takes a while to deploy the function app. When it is done you can test the deployed function in Apollo Studio Sandbox. Open https://studio.apollographql.com/sandbox/explorer in the browser and click the cogwheel button to open the sandbox settings. Change the endpoint to the deployed function URL and add the x-functions-key header with the copied value from Functions -\u0026gt; App keys.\nAfter saving changes you should be connected to the deployed function app.\nSummary After spending a few minutes you got a fully working API with all CRUD methods and even more. It is just the beginning. You can extend the generated model with custom resolvers and role-based authorization and we are getting closer to a rich API build pretty quickly.\n","permalink":"https://rithala.github.io/posts/generate-azure-function-graphql-api-from-database/","summary":"I remember when I first used a GraphQL API in a React SPA. It was a totally different experience from the one I was used to. The flexibility and ease of API discovery with GraphiQL blew my mind. I was using and creating ODATA APIs by then and my first thought was these must be pretty much the same. In fact, the concept is similar. Give front-end developers an API that will allow them to make requests as they want, and get only the data that they want.","title":"Generate Azure Function GraphQL API From Database"},{"content":"Microsoft announced the change of the web address of the Power Automate portal from https://flow.microsoft.com to https://make.powerautomate.com.\nThe latest version of the Power Automate Tools extension introduced support for opening the JSON editor from the new Power Automate portal and Power Apps portal as well.\nYou can learn more about the extension on it\u0026rsquo;s GitHub page, Chrome Web Store or on the Edge Add-ons listing.\n","permalink":"https://rithala.github.io/posts/new-power-automate-tools-version/","summary":"Microsoft announced the change of the web address of the Power Automate portal from https://flow.microsoft.com to https://make.powerautomate.com.\nThe latest version of the Power Automate Tools extension introduced support for opening the JSON editor from the new Power Automate portal and Power Apps portal as well.\nYou can learn more about the extension on it\u0026rsquo;s GitHub page, Chrome Web Store or on the Edge Add-ons listing.","title":"New Power Automate Tools Version"},{"content":"Power Platform custom connectors enable using your API in Power Apps or Power Automate flows. When the API which you plan to integrate with your app is a stable and finished product, then creating a custom connector is a one-time task to do. However, when the API is a part of a developed solution or is under active development then keeping the connector in sync with the API is a cumbersome task.\nIf you are already familiar with DevOps principles then \u0026ldquo;Continous Integration (CI)\u0026rdquo; and \u0026ldquo;Continous Deployment (CD)\u0026rdquo; terms should come to your mind. What we try to achieve here is to get the latest definition of the API after its update, and prepare the custom connector (CI). After that, push the changes to a Power Platform environment (CD).\nBuilding the deployment pipeline How to do this? In this article, I use Azure DevOps pipelines and Power Platform Connectors CLI (paconn) to achieve this. The process should be similar for other build automation software like Jenkins or GitHub Actions.\nCreate the custom connector in the Power Automate or Power Apps portal Wait, we are talking about automation, and then I need to create something manually? Yes, you can create the connector from the scratch using CLI. However, making it in the UI is much more simple. The automation process will take care of updating the connector, and that is the most important for us.\nFollow the official guide to create the custom connector.\nAfter saving the connector, copy the ID from the URL. Look for a value similar to shared_test-5f4540e4010cf8ad28-5fb7527efc5ed37b29 in between /connections/available/custom/ and /edit/. This value is URL encoded. Please remember to decode it before future usage.\nInstalling Power Platform Connectors CLI Installation is very easy. The CLI is a Python app available in PIP.\npip install paconn How to run it in Azure DevOps?\n- script: pip install paconn displayName: Ensure PACONN Authentication This is the tricky part. At the moment of writing this post, paconn does not support userless authentication flows. So we need to help it a little by creating a simple python script that obtains access token to Azure Service Management using a user/password combination (Resource Owner Password Credential).\nCurrently, there is no way to get the access token as an app (Client Credentials) which is why the script uses a service account for authentication. Make sure the account has got an Environment Maker role in the target Power Platform environment.\nYou can use your own Azure AD app registration for the deployment by passing \u0026ldquo;Client ID\u0026rdquo; parameter to the script. If not provided the default app of CLI is used. Basicaly, this is the same as in Azure CLI üòâ If you want to use your app, make sure it has got permissions to https://management.azure.com/user_impersonation scope.\n# authenticate-user.py import argparse import os import adal from msrestazure.azure_active_directory import AADTokenCredentials from paconn.authentication.tokenmanager import TokenManager from paconn.settings.settings import Settings from paconn.common.util import get_config_dir parser = argparse.ArgumentParser( description=\u0026#39;Login to Azure Management using ROPC.\u0026#39;) parser.add_argument(\u0026#39;--user\u0026#39;, action=\u0026#39;store\u0026#39;, type=str, required=True) parser.add_argument(\u0026#39;--password\u0026#39;, action=\u0026#39;store\u0026#39;, type=str, required=True) parser.add_argument(\u0026#39;--clientid\u0026#39;, action=\u0026#39;store\u0026#39;, type=str) parser.add_argument(\u0026#39;--tenant\u0026#39;, action=\u0026#39;store\u0026#39;, type=str) args = parser.parse_args() os.makedirs(get_config_dir(), exist_ok=True) # Pass nones as it is not needed here settings = Settings( connector_id=None, environment=None, api_properties=None, api_definition=None, icon=None, script=None, powerapps_url=None, powerapps_api_version=None ) tenant = args.tenant or settings.tenant client_id = args.clientid or settings.client_id auth_context = adal.AuthenticationContext( authority=settings.authority_url + tenant, api_version=None ) token = auth_context.acquire_token_with_username_password( resource=settings.resource, username=args.user, password=args.password, client_id=client_id ) credentials = AADTokenCredentials( token=token, client_id=client_id) tokenmanager = TokenManager() tokenmanager.write(credentials.token) Of course, you have to run it in the pipeline.\n- task: PythonScript@0 displayName: Login to Azure Management with ROPC inputs: scriptSource: \u0026#39;filePath\u0026#39; scriptPath: \u0026#39;$(Pipeline.Workspace)/drop/connector/authenticate-user.py\u0026#39; arguments: \u0026#39;--user \u0026#34;$(pp_connector_user)\u0026#34; --password \u0026#34;$(pp_connector_password)\u0026#34; --tenant \u0026#34;$(az_ad_tenant_id)\u0026#34;\u0026#39; Connector deployment Finally, it is time to deploy the connector. I used PowerShell, but you can use any scripting language. The steps are:\nDownload the connector definition from the Power Platform environment Fetch the latest Open API definition of your API (only ver. 2 - Swagger 2.0 is supported currently) Update the connector with a new API definition Clean up the stored credentials # update-connector.ps1 param ( [string]$ApiDefinitionUrl, # something like \u0026#34;https://webappname.azurewebsites.net/swagger/v1/swagger.json\u0026#34; [string]$EnvId, # Power Platform environment ID [string]$ConnectorId # The value copied from the URL after you created the connector ) Write-Host \u0026#34;1/4 Getting connector definition\u0026#34; paconn.exe download -e $EnvId -c $ConnectorId Set-Location -Path \u0026#34;.\\$ConnectorId\u0026#34; Write-Host \u0026#34;2/4 Updating swagger file\u0026#34; # Add a few retries here, the app needs some time to warm up straight after the deployment. Invoke-RestMethod -Method Get -Uri $ApiDefinitionUrl -OutFile \u0026#34;apiDefinition.swagger.json\u0026#34; -MaximumRetryCount 5 -TimeoutSec 30 -RetryIntervalSec 15 Write-Host \u0026#34;3/4 Updating connector\u0026#34; paconn.exe update -s .\\settings.json Write-Host \u0026#34;4/4 Cleaning creadentials\u0026#34; Remove-Item \u0026#34;$env:USERPROFILE\\.paconn\u0026#34; -Recurse -Force Let\u0026rsquo;s run it!\n- task: PowerShell@2 displayName: Update connector inputs: filePath: \u0026#39;$(Pipeline.Workspace)/drop/connector/update-connector.ps1\u0026#39; arguments: \u0026#39;-ApiDefinitionUrl $(az_api_swagger_url) -EnvId \u0026#34;$(pp_env_id)\u0026#34; -ConnectorId \u0026#34;$(pp_connector_id)\u0026#34;\u0026#39; workingDirectory: \u0026#39;$(Pipeline.Workspace)/drop/connector\u0026#39; Putting it all together Now it is time to see what the final pipeline looks like.\n# pipeline.yml # Main API and custom connector CICD pipeline file trigger: - develop - master variables: - group: pipeline-variables stages: - stage: build displayName: Build Stage variables: - name: project_path value: $(Build.SourcesDirectory)/api/ - name: connector_path value: $(Build.SourcesDirectory)/connector/ - name: build_configuration value: Release - name: build_platform value: Any CPU - name: runtime value: linux-x64 - name: vmName value: ubuntu-latest jobs: - job: build_api displayName: Build API and Connector pool: vmImage: \u0026#39;$(vmName)\u0026#39; steps: # Here are omitted tasks for building the API package - task: CopyFiles@2 inputs: SourceFolder: \u0026#39;$(connector_path)\u0026#39; Contents: \u0026#39;**\u0026#39; TargetFolder: \u0026#39;$(Build.ArtifactStagingDirectory)/connector\u0026#39; - task: PublishBuildArtifacts@1 inputs: PathtoPublish: \u0026#39;$(Build.ArtifactStagingDirectory)\u0026#39; ArtifactName: \u0026#39;drop\u0026#39; publishLocation: \u0026#39;Container\u0026#39; - deployment: deployment_api displayName: Deploy API pool: vmImage: \u0026#39;ubuntu-latest\u0026#39; environment: env-name strategy: runOnce: deploy: steps: - download: current artifact: drop - template: api-deployment.yml parameters: az_subscription: $(az_subcription_test) - deployment: deployment_connector displayName: Deploy PA Connector pool: vmImage: \u0026#39;windows-latest\u0026#39; environment: env-name dependsOn: deployment_api condition: succeeded() strategy: runOnce: deploy: steps: - download: current artifact: drop - template: connector-deployment.yml # connector-deployment.yml steps: - task: UsePythonVersion@0 displayName: Use Python 3 inputs: versionSpec: \u0026#39;3.x\u0026#39; addToPath: true architecture: \u0026#39;x64\u0026#39; - script: pip install paconn displayName: Ensure PACONN - task: PythonScript@0 displayName: Login to Azure Management with ROPC inputs: scriptSource: \u0026#39;filePath\u0026#39; scriptPath: \u0026#39;$(Pipeline.Workspace)/drop/connector/authenticate-user.py\u0026#39; arguments: \u0026#39;--user \u0026#34;$(pp_connector_user)\u0026#34; --password \u0026#34;$(pp_connector_password)\u0026#34; --tenant \u0026#34;$(az_ad_tenant_id)\u0026#34;\u0026#39; - task: PowerShell@2 displayName: Update connector inputs: filePath: \u0026#39;$(Pipeline.Workspace)/drop/connector/update-connector.ps1\u0026#39; arguments: \u0026#39;-ApiDefinitionUrl $(az_api_swagger_url) -EnvId \u0026#34;$(pp_env_id)\u0026#34; -ConnectorId \u0026#34;$(pp_connector_id)\u0026#34;\u0026#39; workingDirectory: \u0026#39;$(Pipeline.Workspace)/drop/connector\u0026#39; Summary Automating a custom connector deployment allows a development team to work more efficiently. Every new API endpoint is ready to use in your Power App or Power Automate just a few minutes later after pushed changes. There is a list of key things to remember:\nMake sure the service account has got an Environment Maker role in the target Power Platform environment Store the credentials securely e.g., variables group in Azure DevOps and make them \u0026ldquo;secret\u0026rdquo; type. You can use the Azure Pipelines Teams connector to notify the team about a new connector version. To make the changes available in a canvas app you need to remove and add again the connectorW ","permalink":"https://rithala.github.io/posts/automate-deployments-of-custom-power-platform-connectors/","summary":"Power Platform custom connectors enable using your API in Power Apps or Power Automate flows. When the API which you plan to integrate with your app is a stable and finished product, then creating a custom connector is a one-time task to do. However, when the API is a part of a developed solution or is under active development then keeping the connector in sync with the API is a cumbersome task.","title":"Automate Power Platform custom connectors deployment"},{"content":"Today Microsoft released the latest version of SPFx (v1.15). This version comes with the feature that I waited most for (and probably many SharePoint developers). This is a possibility to create a \u0026ldquo;native\u0026rdquo; list item custom form.\nUntil this day, it was a very hacky process of making a custom webpart and redirecting users to it by using custom actions or list view JSON customizers. Now, this development model is fully supported by SPFx.\nIn this article, you will get to know how to build a simple form, test it, and deploy it to the site.\nHow to create a form customizer component? Update the SPFx generator to the newest version\nnpm install @microsoft/generator-sharepoint@latest --global Create a new directory for the project and open it in a terminal then run the command\nyo @microsoft/sharepoint Follow the instructions on the screen. The new customization type is available under Extension -\u0026gt; Form Customizer. In this example, I choose to create a React form.\nCreated SPFx project Now the generator will create the project and install all dependencies. When it is done a standard SPFx project structure is created. You can find the customizer in the src/extensions/{customizerName} directory.\nLet\u0026rsquo;s jump to the customizer component and see what we got here.\nIt looks very similar to the web part component. There are onInit for initialization, render for rendering the form and onDispose for a clean-up. The new things are displayMode which indicates if it is the display, edit or the new mode, self-explanatory properties like list, contentType or itemId, and formSaved() with formClosed() to tell the SharePoint that we finished the job in our form.\nAs I selected React as my framework, the React form component was created as well. By default, it is a pretty empty place.\nCreating a simple form As the form was created, now it is time to add some logic to it. In this example, there is a very simple list with just a title field. SPFx does not give you an API to get, create or edit a list item. Of course, there is SPHttpClient in the component context and you can build requests to SharePoint REST manually, but it is a verbose task. The more friendly way to do it is to use the @pnp/sp library. Add this library to your project.\nnpm install @pnp/sp --save The library has to be initialized. The best place to do it is the onInit() method of the form customizer.\n// MyFirstSpFxFormFormCustomizer.ts import {spfi, SPFx, SPFI} from \u0026#39;@pnp/sp\u0026#39; ... class MyFirstSpFxFormFormCustomizer extends BaseFormCustomizer\u0026lt;IMyFirstSpFxFormFormCustomizerProperties\u0026gt; { private sp: SPFI; public onInit(): Promise\u0026lt;void\u0026gt; { this.sp = spfi().using(SPFx({ pageContext: this.context.pageContext })); return Promise.resolve(); } } Pass the newly created @pnp/sp instance to the React component in the render() method.\n// MyFirstSpFxFormFormCustomizer.ts ... public render(): void { // Use this method to perform your custom rendering. const myFirstSpFxForm: React.ReactElement\u0026lt;{}\u0026gt; = React.createElement( MyFirstSpFxForm, { context: this.context, displayMode: this.displayMode, sp: this.sp, onSave: this._onSave, onClose: this._onClose, } as IMyFirstSpFxFormProps ); ReactDOM.render(myFirstSpFxForm, this.domElement); } The whole form logic is in the React form component. These days I prefer to work with functional components with hooks. So my component now looks like this.\n// MyFirstSpFxForm.tsx export interface IMyFirstSpFxFormProps { context: FormCustomizerContext; displayMode: FormDisplayMode; sp: SPFI; onSave: () =\u0026gt; void; onClose: () =\u0026gt; void; } export default function MyFirstSpFxForm({ context, displayMode, sp, onSave, onClose, }: IMyFirstSpFxFormProps) { return \u0026lt;div className={styles.myFirstSpFxForm} /\u0026gt;; } It is much simpler now. Add some generic form logic to it, and it should be similar to this.\n// MyFirstSpFxForm.tsx import { FormDisplayMode } from \u0026#39;@microsoft/sp-core-library\u0026#39;; import { FormCustomizerContext } from \u0026#39;@microsoft/sp-listview-extensibility\u0026#39;; import * as React from \u0026#39;react\u0026#39;; import { DefaultButton, PrimaryButton } from \u0026#39;office-ui-fabric-react/lib/Button\u0026#39;; import { Stack } from \u0026#39;office-ui-fabric-react/lib/Stack\u0026#39;; import { TextField } from \u0026#39;office-ui-fabric-react/lib/TextField\u0026#39;; import { SPFI } from \u0026#39;@pnp/sp\u0026#39;; import \u0026#39;@pnp/sp/items\u0026#39;; import \u0026#39;@pnp/sp/lists/web\u0026#39;; import \u0026#39;@pnp/sp/webs\u0026#39;; export interface IMyFirstSpFxFormProps { context: FormCustomizerContext; displayMode: FormDisplayMode; sp: SPFI; onSave: () =\u0026gt; void; onClose: () =\u0026gt; void; } interface MyListItem { Id?: number; Title?: string; \u0026#39;odata.etag\u0026#39;?: string; } export default function MyFirstSpFxForm({ context, displayMode, sp, onSave, onClose, }: IMyFirstSpFxFormProps) { const [listItem, setListItem] = React.useState\u0026lt;MyListItem\u0026gt;({}); const isViewForm = React.useMemo( () =\u0026gt; displayMode === FormDisplayMode.Display, [displayMode] ); const onInputChange = React.useMemo( () =\u0026gt; (e: any, value?: string) =\u0026gt; setListItem({ ...listItem, Title: value }), [listItem, setListItem] ); const saveForm = React.useMemo( () =\u0026gt; () =\u0026gt; { (async () =\u0026gt; { const listItems = sp.web.lists.getById( context.list.guid.toString() ).items; const update = { Title: listItem.Title }; if (!listItem.Title) { return alert(\u0026#39;Title is required\u0026#39;); } try { if (!context.itemId) { await listItems.add(update); } else { await listItems .getById(context.itemId) .update(update, listItem[\u0026#39;odata.etag\u0026#39;]); } onSave(); } catch (error) { alert(\u0026#39;Error during saving the list item\u0026#39;); } })(); }, [listItem, context.itemId, context.list.guid, sp] ); React.useEffect(() =\u0026gt; { (async () =\u0026gt; { if (context.itemId) { const item = await sp.web.lists .getById(context.list.guid.toString()) .items.getById(context.itemId)\u0026lt;MyListItem\u0026gt;(); setListItem(item); } else { setListItem({ Title: \u0026#39;\u0026#39; }); } })(); }, [context.itemId, context.list.guid, displayMode, sp]); return ( \u0026lt;Stack tokens={{ childrenGap: 15, padding: 15, }} styles={{ root: { maxWidth: 500 } }} \u0026gt; \u0026lt;span\u0026gt;ID: {context.itemId || \u0026#39;new\u0026#39;}\u0026lt;/span\u0026gt; \u0026lt;TextField label=\u0026#34;Title\u0026#34; value={listItem.Title} onChange={onInputChange} readOnly={isViewForm} /\u0026gt; \u0026lt;Stack horizontal tokens={{ childrenGap: 5, }} horizontalAlign=\u0026#34;end\u0026#34; \u0026gt; {!isViewForm \u0026amp;\u0026amp; \u0026lt;PrimaryButton onClick={saveForm}\u0026gt;Save\u0026lt;/PrimaryButton\u0026gt;} \u0026lt;DefaultButton onClick={onClose}\u0026gt;Close\u0026lt;/DefaultButton\u0026gt; \u0026lt;/Stack\u0026gt; \u0026lt;/Stack\u0026gt; ); } Debugging the form in a browser The basic form is ready, and we are eager on testing it. Before trying out how the customizer works you need to change the serve configuration. To do this, edit the config/serve.json file and replace all occurrences of https://contoso.sharepoint.com/sites/mySite with the URL of your test site and /sites/mySite/Lists/MyList with the server relative URL of the test list that you are using. Your final config should be similar to this.\nNow you can run the project with\nnpx gulp serve The browser should open a development page asking for allowing debug scripts.\nAfter allowing debug scripts our form should be opened and ready to add a new item.\nFill out the title field and click the save button. The form should be closed and a new item is added to the list. So, we got the first big milestone here! The form has created a list item.\nLet\u0026rsquo;s check if the form works correctly in the \u0026ldquo;View\u0026rdquo; mode. Stop the running serve command and run the view form debugging.\nnpx gulp serve --config=myFirstSpFxForm_ViewForm Another successful test! So the last one left. You can try if you can edit an item. Again, stop the serve and run:\nnpx gulp serve --config=myFirstSpFxForm_EditForm The item loads, now try to edit it and save changes.\nIt works! This is your first SPFx form customizer.\nCustomizer deployment To deploy the component you need a content type and update its custom form-related properties. Microsoft introduced these new properties on the content type model:\nProperty Description NewFormClientSideComponentId component id for new form NewFormClientSideComponentProperties optional configuration details EditFormClientSideComponentId component id display form EditFormClientSideComponentProperties optional configuration details DisplayFormClientSideComponentId component id display form DisplayFormClientSideComponentProperties optional configuration details You can use various ways to change the content type properties.\nContent-type as part of SPFx package, Content-type as part of PnP provisioning template, Changing content-type properties with REST or CSOM, PnP PowerShell (limited). In this article, I will show you how to use the last option on the list above. In fact, it is a combination of using PnP PowerShell to get the access token and making an HTTP request to SharePoint REST API manually. For now, there is no built-in method supporting new content type properties.\nThe deployment starts with building the SPFx solution. The SPFx has some very strict linter rules that will stop us from packaging this solution. For this simple PoC purpose add this line to gulpfile.js. Please do not use this in a production environment.\nbuild.addSuppression(/.*/g); Next, you can run this command.\nnpx gulp bundle --ship \u0026amp;\u0026amp; npx gulp package-solution --ship You can find the build package in the sharepoint/package-name.sppkg directory. Upload the package to a tenant or site-scoped app catalog.\nTo make the customizer on a SharePoint site you need to add the app. On the home page click the \u0026ldquo;new\u0026rdquo; button and select \u0026ldquo;App\u0026rdquo;.\nOn the next page click the \u0026ldquo;Add\u0026rdquo; button next to the solution.\nWhen you will see the success alert, go to site settings and select the content-types option. Then create a new content type.\nAfter creating a content type, copy the content-type ID value from the confirmation page.\nLeave the browser for a moment, now it is time for some PowerShell. Connect to a SharePoint site.\nConnect-PnPOnline -Url https://tenantname.sharepoint.com/sites/SiteName -Interactive Get the created content type ID and the component ID from the MyFormCustomizer.manfiest.json file\n$contentTypeId = \u0026#34;0x01003D4C996DABD4694BA215125F3C268050\u0026#34; $componentId = \u0026#34;83b57037-dfde-4ad3-9ab0-0274c6e26193\u0026#34; $token = Get-PnPAccessToken -ResourceUrl \u0026#34;https://tenantname.sharepoint.com/.default\u0026#34; $body = @{NewFormClientSideComponentId = $componentId; EditFormClientSideComponentId = $componentId; DisplayFormClientSideComponentId = $componentId} | ConvertTo-Json Invoke-RestMethod -Method Patch -Uri \u0026#34;https://tenantname.sharepoint.com/sites/SiteName/_api/web/contenttypes(\u0026#39;$contentTypeId\u0026#39;)\u0026#34; -Headers @{ Authorization = \u0026#34;Bearer $token\u0026#34;; \u0026#34;Content-Type\u0026#34; = \u0026#34;application/json\u0026#34; } -Body $body You can close the PowerShell window. Assign the newly created content type to a list. To do this go to List settings -\u0026gt; Advanced settings. Change \u0026ldquo;Allow management of content types?\u0026rdquo; to \u0026ldquo;Yes\u0026rdquo; and save changes.\nNow \u0026ldquo;Content Types\u0026rdquo; section in the list settings is visible. Click the \u0026ldquo;Add from existing site content types\u0026rdquo; action. On the next page select the previously created content type and click \u0026ldquo;Ok\u0026rdquo;.\nIt is a good idea to hide the default content type. To do this click the \u0026ldquo;Change new button order and default content type\u0026rdquo; link and change the settings to the following.\nNow you can try it out by creating a new item. The freshly created form customizer should be your new list form ü•≥\nThe tooling at this stage is a little cumbersome. Nevertheless, today is the go-live day of this feature üëç\n","permalink":"https://rithala.github.io/posts/build-and-deploy-spfx-form-customizer/","summary":"Today Microsoft released the latest version of SPFx (v1.15). This version comes with the feature that I waited most for (and probably many SharePoint developers). This is a possibility to create a \u0026ldquo;native\u0026rdquo; list item custom form.\nUntil this day, it was a very hacky process of making a custom webpart and redirecting users to it by using custom actions or list view JSON customizers. Now, this development model is fully supported by SPFx.","title":"Build and deploy SPFx form customizer"},{"content":" Let\u0026rsquo;s assume you need to summarize the sales data per salesman and prepare a monthly report that will be sent to the manager.\nHow can you do this in Power Automate? The first thing that may come to your mind is to use SharePoint\u0026rsquo;s \u0026ldquo;Get Items\u0026rdquo; action to get all list items for the desired time frame and then calculate the sums yourself in the \u0026ldquo;Apply to each\u0026rdquo; loop. It may work for a small list but it will fail when we get to a larger number of items. Firstly, because of the action maximum items threshold (5000). Secondly, it will take ages to calculate that.\nIn this article, I present the solution that offloads the calculations to the SharePoint API using the \u0026ldquo;renderListDataAsStream\u0026rdquo; method. Instead of calculating the aggregations in a flow, the SharePoint API returns already aggregated data.\nRender List Data As Stream REST Method First of all, let\u0026rsquo;s get to know the tools which we will be using. \u0026ldquo;RednderListDataAsStream\u0026rdquo; is the SharePoint REST API method that allows retrieving list items using a CAML query. This approach is the most flexible way to query SharePoint lists data and is widely used by SharePoint\u0026rsquo;s team to build the SharePoint UI. You can learn more about it in a great article written by my colleague Ewelina. The endpoint is available under the following URL.\n[POST] _api/web/lists/getByTitle(\u0026#39;\u0026lt;list name\u0026gt;\u0026#39;)/RenderListDataAsStream CAML Aggregations If you are familiar with SQL aggregations then CAML aggregations should be pretty straightforward for you. In general, aggregations allow summarizing data as a metric. In CAML you can use one of the following aggregations:\nAggreagation Description COUNT Count list items AVG Average MAX Maximum MIN Minimum SUM Sum STDEV Standard deviation VAR Variance How do you use it in a CAML query? Here is an example of a CAML query summarizing data in the \u0026ldquo;OrderValue\u0026rdquo; field.\n\u0026lt;View\u0026gt; \u0026lt;Aggregations Value=\u0026#34;On\u0026#34;\u0026gt; \u0026lt;FieldRef Name=\u0026#34;OrderValue\u0026#34; Type=\u0026#34;SUM\u0026#34; /\u0026gt; \u0026lt;/Aggregations\u0026gt; \u0026lt;ViewFields\u0026gt; \u0026lt;FieldRef Name=\u0026#34;OrderValue\u0026#34; /\u0026gt; \u0026lt;/ViewFields\u0026gt; \u0026lt;RowLimit\u0026gt;1\u0026lt;/RowLimit\u0026gt; \u0026lt;/View\u0026gt; The query above returns the sum of the order value in all list items (some fields are omitted for brevity).\n{ \u0026#34;Row\u0026#34;: [ { \u0026#34;OrderValue.SUM\u0026#34;: \u0026#34;16,239,943.51 z≈Ç\u0026#34; } ] } Often, you may want to receive the sum for a group. In our case let\u0026rsquo;s assume we want a sum of the order value (OrderValue) column per product category (ProductCategory).\nWhen you are grouping the items it is important to remember how will it behave in a paged response which is the default behavior of this endpoint. You always have to match the \u0026ldquo;GroupLimit\u0026rdquo; attribute of the GroupBy element with the value of the row limit element.\n\u0026lt;View\u0026gt; \u0026lt;Aggregations Value=\u0026#34;On\u0026#34;\u0026gt; \u0026lt;FieldRef Name=\u0026#34;OrderValue\u0026#34; Type=\u0026#34;SUM\u0026#34; /\u0026gt; \u0026lt;/Aggregations\u0026gt; \u0026lt;Query\u0026gt; \u0026lt;GroupBy Collapse=\u0026#34;TRUE\u0026#34; GroupLimit=\u0026#34;20\u0026#34;\u0026gt; \u0026lt;FieldRef Name=\u0026#34;ProductCategory\u0026#34; /\u0026gt; \u0026lt;/GroupBy\u0026gt; \u0026lt;/Query\u0026gt; \u0026lt;ViewFields\u0026gt; \u0026lt;FieldRef Name=\u0026#34;ProductCategory\u0026#34; /\u0026gt; \u0026lt;FieldRef Name=\u0026#34;OrderValue\u0026#34; /\u0026gt; \u0026lt;/ViewFields\u0026gt; \u0026lt;RowLimit Paged=\u0026#34;TRUE\u0026#34;\u0026gt;20\u0026lt;/RowLimit\u0026gt; \u0026lt;/View\u0026gt; Then the response looks like this. The aggregated value is returned in FieldName.AggreagtionFunction.agg.\n{ \u0026#34;Row\u0026#34;: [ { \u0026#34;ProductCategory\u0026#34;: [ { \u0026#34;lookupId\u0026#34;: 1, \u0026#34;lookupValue\u0026#34;: \u0026#34;Beverages\u0026#34; } ], \u0026#34;ProductCategory.singleurlencoded\u0026#34;: \u0026#34;Beverages\u0026#34;, \u0026#34;OrderValue.SUM.agg\u0026#34;: \u0026#34;2,123,446.16 z≈Ç\u0026#34; }, { \u0026#34;ProductCategory\u0026#34;: [ { \u0026#34;lookupId\u0026#34;: 2, \u0026#34;lookupValue\u0026#34;: \u0026#34;Condiments\u0026#34; } ], \u0026#34;ProductCategory.singleurlencoded\u0026#34;: \u0026#34;Condiments\u0026#34;, \u0026#34;OrderValue.SUM.agg\u0026#34;: \u0026#34;1,813,982.89 z≈Ç\u0026#34; }, { \u0026#34;ProductCategory\u0026#34;: [ { \u0026#34;lookupId\u0026#34;: 3, \u0026#34;lookupValue\u0026#34;: \u0026#34;Confections\u0026#34; } ], \u0026#34;ProductCategory.singleurlencoded\u0026#34;: \u0026#34;Confections\u0026#34;, \u0026#34;OrderValue.SUM.agg\u0026#34;: \u0026#34;2,092,854.43 z≈Ç\u0026#34; } ] } How to use it in a Power Automate flow? Now that you know the theory it is time to use it. What was the assumption that the article starts with?\nLet\u0026rsquo;s assume you need to summarize the sales data per salesman and prepare a monthly report that will be sent to the manager.\nSo we start with a blank flow with a schedule trigger that runs every month.\nNext, create variables for a time frame.\nNow is the crucial part, you got to prepare the CAML query. The most convenient way is to use the Compose action. Make your job easier by using single quotes for attribute values. Then you do not need to escape it when sending it in a JSON body.\nIt is time to send the request to the API. Use the \u0026ldquo;Send an HTTP request to SharePoint REST\u0026rdquo; action, set the method parameter to POST and use the described render list data as a stream method for a selected list.\nYou can add the Accept header with application/json;odata.metadata=none value to omit the ODATA metadata in the response.\nAlso, when you use \u0026ldquo;renderListDataAsStream\u0026rdquo; is nice to add the DatesInUtc parameter. Then you are sure that the service account time zone setting does not affect your result.\nThe last task on our list is to send the collected data as a mail report.\nI recommend testing the flow at this stage and copying the JSON response. Then add the Parse JSON action, and use the copied JSON to generate a JSON schema. This will help in working with the action\u0026rsquo;s response in the editor.\nTo make the flow even faster use Select and Create HTML table operations instead of Apply to each to build the report content.\nNow just send the report as an email.\nThat\u0026rsquo;s it! What does the report look like? For sure we can play around with the HTML to make it look better, but should be enough for MVP üôÇ\n","permalink":"https://rithala.github.io/posts/sharepoint-list-items-aggregations-in-power-automate/","summary":"Let\u0026rsquo;s assume you need to summarize the sales data per salesman and prepare a monthly report that will be sent to the manager.\nHow can you do this in Power Automate? The first thing that may come to your mind is to use SharePoint\u0026rsquo;s \u0026ldquo;Get Items\u0026rdquo; action to get all list items for the desired time frame and then calculate the sums yourself in the \u0026ldquo;Apply to each\u0026rdquo; loop. It may work for a small list but it will fail when we get to a larger number of items.","title":"How to aggregate (SUM, AVG, COUNT etc.) SharePoint list items' data efficient in Power Automate"},{"content":"I bet every PowerAutomate consultant wonders if there is an easy way to replace all SharePoint site URLs or variables names in a flow definition ü§î I could not find the answer, so I built this small (my firstüòä) Chrome/Edge extension allowing editing a flow as JSON in a web version of VS Code. Please, share this with your colleagues and feedback is much appreciated üòâ\nLink to GitHub\nLink to the Chrome Web Store Link to the Edge Add-ons listing\n","permalink":"https://rithala.github.io/posts/power-automate-chrome-extension/","summary":"I bet every PowerAutomate consultant wonders if there is an easy way to replace all SharePoint site URLs or variables names in a flow definition ü§î I could not find the answer, so I built this small (my firstüòä) Chrome/Edge extension allowing editing a flow as JSON in a web version of VS Code. Please, share this with your colleagues and feedback is much appreciated üòâ\nLink to GitHub\nLink to the Chrome Web Store Link to the Edge Add-ons listing","title":"Power Automate Chrome Extension"}]